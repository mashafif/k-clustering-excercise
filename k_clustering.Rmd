---
title: "K-clustering problem"
author: "Afif"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This mini-project is based on the K-Means exercise from 'R in Action'
Go here for the original blog post and solutions
http://www.r-bloggers.com/k-means-clustering-from-r-in-action/

## Exercise 0: 
Install these packages if you don't have them already

```{r, eval=FALSE}
install.packages(c("cluster", "rattle.data","NbClust"))
```
Now load the data and look at the first few rows
```{r}
data(wine, package="rattle.data")
head(wine)
```
## Exercise 1: 
Remove the first column from the data and scale it using the scale() function

```{r}
df <- scale(wine[-1])
```

Now we'd like to cluster the data using K-Means. 
How do we decide how many clusters to use if you don't know that already?
We'll try two methods.

## Method 1: Elbow Method 
A plot of the total within-groups sums of squares against the 
number of clusters in a K-means solution can be helpful. A bend in the 
graph can suggest the appropriate number of clusters. 

```{r}
wssplot <- function(data, nc=15, seed=1234){
	              wss <- (nrow(data)-1)*sum(apply(data,2,var))
               	      for (i in 2:nc){
		        set.seed(seed)
	                wss[i] <- sum(kmeans(data, centers=i)$withinss)}
	                
		      plot(1:nc, wss, type="b", xlab="Number of Clusters",
	                        ylab="Within groups sum of squares")
	   }

wssplot(df)
```

## Exercise 2:
*How many clusters does this method suggest?*

**Answer**: 3 to 4 clusters, according to above method.

*Why does this method work? What's the intuition behind it?*

**Answer**: Purpose of this method is to compare the within groups sums of squares of each k-means solution with different number of cluster. "Within group of sum squares" signifies how spread out/how big is the variance of the members of each cluster with its cluster's means or centroid. This means the lower the within group of squares the less spread out the members of the clusters, the better the result is. By plotting the relation between number of clusters and within groups of squares, 
we can find after how many number of clusters yield the most significance improvement in within group of squares, which in this case specified by a bend in the graph.


## Method 2: Use the NbClust library 
NbClust library runs many experiments and gives a distribution of potential number of clusters.

```{r}
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
barplot(table(nc$Best.n[1,]),
	          xlab="Numer of Clusters", ylab="Number of Criteria",
		            main="Number of Clusters Chosen by 26 Criteria")
```


##Exercise 3: 
*How many clusters does this method suggest?*

**Answer**: 3 clusters

##Exercise 4: 
Once you've picked the number of clusters, run k-means using this number of clusters. Output the result of calling kmeans() into a variable fit.km

```{r}
set.seed(1234)
fit.km <- kmeans(df, 3, nstart=25)
fit.km$cluster
```

Now we want to evaluate how well this clustering does.

##Exercise 5: 
Using the table() function, show how the clusters in comparison to the actual wine types in wine$Type. Would you consider this a good clustering?

```{r}
table(fit.km$cluster, wine$Type)
```

From above result I would consider this as a good clustering due to number of error which is only 6 errors.

##Exercise 6:
Visualize these clusters using  function clusplot() from the cluster library
Would you consider this a good clustering?

```{r}
library(cluster)
clusplot(wine[-1], fit.km$cluster, main="Cluster Plot for Wine Data")
```

According to above clusplot, I would also consider this as good modelling, due to clear separation between each clusters with only a very minor overlap.
